{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this Project,I will be implementing an L-layered deep neural network and train it on the MNIST dataset. The MNIST dataset contains scanned images of handwritten digits, along with their correct classification labels (between 0-9). MNIST's name comes from the fact that it is a modified subset of two data sets collected by NIST, the United States' National Institute of Standards and Technology.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import gzip\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import h5py\n",
    "import sklearn\n",
    "import sklearn.datasets\n",
    "import scipy\n",
    "from PIL import Image\n",
    "from scipy import ndimage\n",
    "\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The MNIST dataset we use here is 'mnist.pkl.gz' which is divided into training, validation and test data. The following function <i> load_data() </i> unpacks the file and extracts the training, validation and test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    f = gzip.open('mnist.pkl.gz', 'rb')\n",
    "    f.seek(0)\n",
    "    training_data, validation_data, test_data = pickle.load(f, encoding='latin1')\n",
    "    f.close()\n",
    "    return (training_data, validation_data, test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how the data looks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data, validation_data, test_data = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.]], dtype=float32),\n",
       " array([5, 0, 4, ..., 8, 4, 8], dtype=int64))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 784)\n",
      "(50000,)\n"
     ]
    }
   ],
   "source": [
    "# shape of data\n",
    "print(training_data[0].shape)\n",
    "print(training_data[1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The feature dataset is:[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "The target dataset is:[5 0 4 ... 8 4 8]\n",
      "The number of examples in the training dataset is:50000\n",
      "The number of points in a single input is:784\n"
     ]
    }
   ],
   "source": [
    "print(\"The feature dataset is:\" + str(training_data[0]))\n",
    "print(\"The target dataset is:\" + str(training_data[1]))\n",
    "print(\"The number of examples in the training dataset is:\" + str(len(training_data[0])))\n",
    "print(\"The number of points in a single input is:\" + str(len(training_data[0][1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The target variable is converted to a one hot matrix. We use the function <i> one_hot </i> to convert the target dataset to one hot encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot(j):\n",
    "    # input is the target dataset of shape (m,) where m is the number of data points\n",
    "    # returns a 2 dimensional array of shape (10, m) where each target value is converted to a one hot encoding\n",
    "    # Look at the next block of code for a better understanding of one hot encoding\n",
    "    n = j.shape[0]\n",
    "    new_array = np.zeros((10, n))\n",
    "    index = 0\n",
    "    for res in j:\n",
    "        new_array[res][index] = 1.0\n",
    "        index = index + 1\n",
    "    return new_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = np.array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n",
    "print(data.shape)\n",
    "one_hot(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function data_wrapper() will convert the dataset into the desired shape and also convert the ground truth labels to one_hot matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_wrapper():\n",
    "    tr_d, va_d, te_d = load_data()\n",
    "    \n",
    "    training_inputs = np.array(tr_d[0][:]).T\n",
    "    training_results = np.array(tr_d[1][:])\n",
    "    train_set_y = one_hot(training_results)\n",
    "    \n",
    "    validation_inputs = np.array(va_d[0][:]).T\n",
    "    validation_results = np.array(va_d[1][:])\n",
    "    validation_set_y = one_hot(validation_results)\n",
    "    \n",
    "    test_inputs = np.array(te_d[0][:]).T\n",
    "    test_results = np.array(te_d[1][:])\n",
    "    test_set_y = one_hot(test_results)\n",
    "    \n",
    "    return (training_inputs, train_set_y, test_inputs, test_set_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set_x, train_set_y, test_set_x, test_set_y = data_wrapper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_set_x shape: (784, 50000)\n",
      "train_set_y shape: (10, 50000)\n",
      "test_set_x shape: (784, 10000)\n",
      "test_set_y shape: (10, 10000)\n"
     ]
    }
   ],
   "source": [
    "print (\"train_set_x shape: \" + str(train_set_x.shape))\n",
    "print (\"train_set_y shape: \" + str(train_set_y.shape))\n",
    "print (\"test_set_x shape: \" + str(test_set_x.shape))\n",
    "print (\"test_set_y shape: \" + str(test_set_y.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the data_wrapper has converted the training and validation data into numpy array of desired shapes. Let's convert the actual labels into a dataframe to see if the one hot conversions are correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = pd.DataFrame(train_set_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The target dataset is:[5 0 4 ... 8 4 8]\n",
      "The one hot encoding dataset is:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>49990</th>\n",
       "      <th>49991</th>\n",
       "      <th>49992</th>\n",
       "      <th>49993</th>\n",
       "      <th>49994</th>\n",
       "      <th>49995</th>\n",
       "      <th>49996</th>\n",
       "      <th>49997</th>\n",
       "      <th>49998</th>\n",
       "      <th>49999</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 50000 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   0      1      2      3      4      5      6      7      8      9      \\\n",
       "0    0.0    1.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
       "1    0.0    0.0    0.0    1.0    0.0    0.0    1.0    0.0    1.0    0.0   \n",
       "2    0.0    0.0    0.0    0.0    0.0    1.0    0.0    0.0    0.0    0.0   \n",
       "3    0.0    0.0    0.0    0.0    0.0    0.0    0.0    1.0    0.0    0.0   \n",
       "4    0.0    0.0    1.0    0.0    0.0    0.0    0.0    0.0    0.0    1.0   \n",
       "5    1.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
       "6    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
       "7    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
       "8    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
       "9    0.0    0.0    0.0    0.0    1.0    0.0    0.0    0.0    0.0    0.0   \n",
       "\n",
       "   ...    49990  49991  49992  49993  49994  49995  49996  49997  49998  49999  \n",
       "0  ...      0.0    0.0    0.0    0.0    0.0    0.0    1.0    0.0    0.0    0.0  \n",
       "1  ...      0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0  \n",
       "2  ...      0.0    0.0    0.0    1.0    0.0    0.0    0.0    0.0    0.0    0.0  \n",
       "3  ...      0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0  \n",
       "4  ...      0.0    0.0    0.0    0.0    1.0    0.0    0.0    0.0    1.0    0.0  \n",
       "5  ...      0.0    1.0    1.0    0.0    0.0    1.0    0.0    0.0    0.0    0.0  \n",
       "6  ...      0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0  \n",
       "7  ...      0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0  \n",
       "8  ...      1.0    0.0    0.0    0.0    0.0    0.0    0.0    1.0    0.0    1.0  \n",
       "9  ...      0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0  \n",
       "\n",
       "[10 rows x 50000 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"The target dataset is:\" + str(training_data[1]))\n",
    "print(\"The one hot encoding dataset is:\")\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let us visualise the dataset. Feel free to change the index to see if the training data has been correctly tagged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x18afe4cfef0>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "index  = 1000\n",
    "k = train_set_x[:,index]\n",
    "k = k.reshape((28, 28))\n",
    "plt.title('Label is {label}'.format(label= training_data[1][index]))\n",
    "plt.imshow(k, cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feedforward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sigmoid\n",
    "This is one of the activation functions. It takes the cumulative input to the layer, the matrix **Z**, as the input. Upon application of the *sigmoid* function, the output matrix **H** is calculated. Also, **Z** is stored as the variable **sigmoid_memory** since it will be later used in backpropagation.You use _[np.exp()](https://docs.scipy.org/doc/numpy/reference/generated/numpy.exp.html)_ here in the following way. The exponential gets applied to all the elements of Z."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(Z):\n",
    "    \n",
    "    # Z is numpy array of shape (n, m) where n is number of neurons in the layer and m is the number of samples \n",
    "    # sigmoid_memory is stored as it is used later on in backpropagation\n",
    "    \n",
    "    H = 1/(1+np.exp(-Z))\n",
    "    sigmoid_memory = Z\n",
    "    \n",
    "    return H, sigmoid_memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sigmoid(Z) = (array([[0.5       , 0.73105858],\n",
      "       [0.88079708, 0.95257413],\n",
      "       [0.98201379, 0.99330715],\n",
      "       [0.99752738, 0.99908895]]), array([[0, 1],\n",
      "       [2, 3],\n",
      "       [4, 5],\n",
      "       [6, 7]]))\n"
     ]
    }
   ],
   "source": [
    "Z = np.arange(8).reshape(4,2)\n",
    "print (\"sigmoid(Z) = \" + str(sigmoid(Z)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### relu\n",
    "This is one of the activation functions. It takes the cumulative input to the layer, matrix **Z** as the input. Upon application of the **relu** function, matrix **H** which is the output matrix is calculated. Also, **Z** is stored as **relu_memory** which will be later used in backpropagation. You use _[np.maximum()](https://docs.scipy.org/doc/numpy/reference/generated/numpy.maximum.html)_ here in the following way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(Z):\n",
    "    # Z is numpy array of shape (n, m) where n is number of neurons in the layer and m is the number of samples \n",
    "    # relu_memory is stored as it is used later on in backpropagation\n",
    "    \n",
    "    H = np.maximum(0,Z)\n",
    "    \n",
    "    assert(H.shape == Z.shape)\n",
    "    \n",
    "    relu_memory = Z \n",
    "    return H, relu_memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "relu(Z) = (array([[ 1,  3],\n",
      "       [ 0,  0],\n",
      "       [ 0,  7],\n",
      "       [ 9, 18]]), array([[ 1,  3],\n",
      "       [-1, -4],\n",
      "       [-5,  7],\n",
      "       [ 9, 18]]))\n"
     ]
    }
   ],
   "source": [
    "Z = np.array([1, 3, -1, -4, -5, 7, 9, 18]).reshape(4,2)\n",
    "print (\"relu(Z) = \" + str(relu(Z)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### softmax\n",
    "This is the activation of the last layer. It takes the cumulative input to the layer, matrix **Z** as the input. Upon application of the **softmax** function, the output matrix **H** is calculated. Also, **Z** is stored as **softmax_memory** which will be later used in backpropagation. You use _[np.exp()](https://docs.scipy.org/doc/numpy/reference/generated/numpy.exp.html)_ and _[np.sum()](https://docs.scipy.org/doc/numpy-1.10.0/reference/generated/numpy.sum.html)_ here in the following way. The exponential gets applied to all the elements of Z."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(Z):\n",
    "    # Z is numpy array of shape (n, m) where n is number of neurons in the layer and m is the number of samples \n",
    "    # softmax_memory is stored as it is used later on in backpropagation\n",
    "   \n",
    "    Z_exp = np.exp(Z)\n",
    "\n",
    "    Z_sum = np.sum(Z_exp,axis = 0, keepdims = True)\n",
    "    \n",
    "    H = Z_exp/Z_sum  #normalising step\n",
    "    softmax_memory = Z\n",
    "    \n",
    "    return H, softmax_memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "Z = np.array([[11,19,10], [12, 21, 23]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2.68941421e-01 1.19202922e-01 2.26032430e-06]\n",
      " [7.31058579e-01 8.80797078e-01 9.99997740e-01]]\n",
      "[[11 19 10]\n",
      " [12 21 23]]\n"
     ]
    }
   ],
   "source": [
    "#Z = np.array(np.arange(30)).reshape(10,3)\n",
    "H, softmax_memory = softmax(Z)\n",
    "print(H)\n",
    "print(softmax_memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### initialize_parameters\n",
    "Let's now create a function **initialize_parameters** which initializes the weights and biases of the various layers. One way to initialise is to set all the parameters to 0. This is not a considered a good strategy as all the neurons will behave the same way and it'll defeat the purpose of deep networks. Hence, we initialize the weights randomly to very small values but not zeros. The biases are initialized to 0. Note that the **initialize_parameters** function initializes the parameters for all the layers in one `for` loop. \n",
    "\n",
    "The inputs to this function is a list named `dimensions`. The length of the list is the number layers in the network + 1 (the plus one is for the input layer, rest are hidden + output). The first element of this list is the dimensionality or length of the input (784 for the MNIST dataset). The rest of the list contains the number of neurons in the corresponding (hidden and output) layers.\n",
    "\n",
    "For example `dimensions = [784, 3, 7, 10]` specifies a network for the MNIST dataset with two hidden layers and a 10-dimensional softmax output.\n",
    "\n",
    "Also, notice that the parameters are returned in a dictionary. This will help you in implementing the feedforward through the layer and the backprop throught the layer at once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters(dimensions):\n",
    "\n",
    "    # dimensions is a list containing the number of neuron in each layer in the network\n",
    "    # It returns parameters which is a python dictionary containing the parameters \"W1\", \"b1\", ..., \"WL\", \"bL\":\n",
    "\n",
    "    np.random.seed(2)\n",
    "    parameters = {}\n",
    "    L = len(dimensions)            # number of layers in the network + 1\n",
    "\n",
    "    for l in range(1, L): \n",
    "        parameters['W' + str(l)] = np.random.randn(dimensions[l], dimensions[l-1]) * 0.1\n",
    "        parameters['b' + str(l)] = np.zeros((dimensions[l], 1)) \n",
    "        \n",
    "        assert(parameters['W' + str(l)].shape == (dimensions[l], dimensions[l-1]))\n",
    "        assert(parameters['b' + str(l)].shape == (dimensions[l], 1))\n",
    "\n",
    "        \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1 = [[-0.04167578 -0.00562668 -0.21361961 ... -0.06168445  0.03213358\n",
      "  -0.09464469]\n",
      " [-0.05301394 -0.1259207   0.16775441 ... -0.03284246 -0.05623108\n",
      "   0.01179136]\n",
      " [ 0.07386378 -0.15872956  0.01532001 ... -0.08428557  0.10040469\n",
      "   0.00545832]]\n",
      "b1 = [[0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "W2 = [[ 0.06650944 -0.19626047  0.2112715 ]\n",
      " [-0.28074571 -0.13967752  0.02641189]\n",
      " [ 0.10925169  0.06646016  0.08565535]\n",
      " [-0.11058228  0.03715795  0.13440124]\n",
      " [-0.16421272 -0.1153127   0.02013163]\n",
      " [ 0.13985659  0.07228733 -0.10717236]\n",
      " [-0.05673344 -0.03663499 -0.15460347]]\n",
      "b2 = [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n"
     ]
    }
   ],
   "source": [
    "dimensions  = [784, 3,7,10]\n",
    "parameters = initialize_parameters(dimensions)\n",
    "print(\"W1 = \" + str(parameters[\"W1\"]))\n",
    "print(\"b1 = \" + str(parameters[\"b1\"]))\n",
    "print(\"W2 = \" + str(parameters[\"W2\"]))\n",
    "print(\"b2 = \" + str(parameters[\"b2\"]))\n",
    "# print(\"W3 = \" + str(parameters[\"W3\"]))\n",
    "# print(\"b3 = \" + str(parameters[\"b3\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### layer_forward\n",
    "\n",
    "The function **layer_forward** implements the forward propagation for a certain layer 'l'. It calculates the cumulative input into the layer **Z** and uses it to calculate the output of the layer **H**. It takes **H_prev, W, b and the activation function** as inputs and stores the **linear_memory, activation_memory** in the variable **memory** which will be used later in backpropagation. \n",
    "\n",
    "<br> You have to first calculate the **Z**(using the forward propagation equation), **linear_memory**(H_prev, W, b) and then calculate **H, activation_memory**(Z) by applying activation functions - **sigmoid**, **relu** and **softmax** on **Z**.\n",
    "\n",
    "<br> Note that $$H^{L-1}$$ is referred here as H_prev. You might want to use _[np.dot()](https://docs.scipy.org/doc/numpy/reference/generated/numpy.dot.html)_ to carry out the matrix multiplication."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def layer_forward(H_prev, W, b, activation = 'relu'):\n",
    "\n",
    "    # H_prev is of shape (size of previous layer, number of examples)\n",
    "    # W is weights matrix of shape (size of current layer, size of previous layer)\n",
    "    # b is bias vector of shape (size of the current layer, 1)\n",
    "    # activation is the activation to be used for forward propagation : \"softmax\", \"relu\", \"sigmoid\"\n",
    "\n",
    "    # H is the output of the activation function \n",
    "    # memory is a python dictionary containing \"linear_memory\" and \"activation_memory\"\n",
    "    \n",
    "    if activation == \"sigmoid\":\n",
    "        Z = np.dot(W, H_prev) + b \n",
    "        linear_memory = (H_prev, W, b)\n",
    "        H, activation_memory = sigmoid(Z)\n",
    " \n",
    "    elif activation == \"softmax\":\n",
    "        Z = np.dot(W, H_prev) + b \n",
    "        linear_memory = (H_prev, W, b)\n",
    "        H, activation_memory = softmax(Z)\n",
    "    \n",
    "    elif activation == \"relu\":\n",
    "        Z = np.dot(W, H_prev) + b\n",
    "        linear_memory = (H_prev, W, b)\n",
    "        H, activation_memory = relu(Z)\n",
    "        \n",
    "    assert (H.shape == (W.shape[0], H_prev.shape[1]))\n",
    "    memory = (linear_memory, activation_memory)\n",
    "\n",
    "    return H, memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.        , 1.        , 1.        , 1.        , 1.        ],\n",
       "       [0.99908895, 0.99330715, 0.99999969, 1.        , 0.99987661],\n",
       "       [0.73105858, 0.5       , 0.99330715, 0.9999546 , 0.88079708]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# verify\n",
    "# l-1 has two neurons, l has three, m = 5\n",
    "# H_prev is (l-1, m)\n",
    "# W is (l, l-1)\n",
    "# b is (l, 1)\n",
    "# H should be (l, m)\n",
    "H_prev = np.array([[1,0, 5, 10, 2], [2, 5, 3, 10, 2]])\n",
    "W_sample = np.array([[10, 5], [2, 0], [1, 0]])\n",
    "b_sample = np.array([10, 5, 0]).reshape((3, 1))\n",
    "\n",
    "H = layer_forward(H_prev, W_sample, b_sample, activation=\"sigmoid\")[0]\n",
    "H"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### L_layer_forward\n",
    "**`L_layer_forward`** performs one forward pass through the whole network for all the training samples (note that we are feeding all training examples in one single batch). Use the **`layer_forward`** you have created above here to perform the feedforward for layers 1 to 'L-1' in the for loop with the activation **relu**. The last layer having a different activation **softmax** is calculated outside the loop. Notice that the **memory** is appended to **memories** for all the layers. These will be used in the backward order during backpropagation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def L_layer_forward(X, parameters):\n",
    "\n",
    "    # X is input data of shape (input size, number of examples)\n",
    "    # parameters is output of initialize_parameters()\n",
    "    \n",
    "    # HL is the last layer's post-activation value\n",
    "    # memories is the list of memory containing (for a relu activation, for example):\n",
    "    # - every memory of relu forward (there are L-1 of them, indexed from 1 to L-1), \n",
    "    # - the memory of softmax forward (there is one, indexed L) \n",
    "\n",
    "    memories = []\n",
    "    H = X\n",
    "    L = len(parameters) // 2                  # number of layers in the neural network\n",
    "    \n",
    "    # Implement relu layer (L-1) times as the Lth layer is the softmax layer\n",
    "    for l in range(1, L):\n",
    "        H_prev = H \n",
    "        \n",
    "        H, memory = layer_forward(H_prev, \n",
    "                                 parameters[\"W\" + str(l)], \n",
    "                                 parameters[\"b\" + str(l)], \n",
    "                                 activation='relu')\n",
    "        memories.append(memory)\n",
    "    \n",
    "    # Implement the final softmax layer\n",
    "    # HL here is the final prediction P as specified in the lectures\n",
    "    HL, memory = layer_forward(H,\n",
    "                              parameters[\"W\" + str(L)], \n",
    "                              parameters[\"b\" + str(L)], \n",
    "                              activation='softmax')\n",
    "    memories.append(memory)\n",
    "\n",
    "    assert(HL.shape == (10, X.shape[1]))\n",
    "            \n",
    "    return HL, memories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(784, 10)\n",
      "[[0.10106734 0.10045152 0.09927757 0.10216656 0.1       ]\n",
      " [0.10567625 0.10230873 0.10170271 0.11250099 0.1       ]\n",
      " [0.09824287 0.0992886  0.09967128 0.09609693 0.1       ]\n",
      " [0.10028288 0.10013048 0.09998149 0.10046076 0.1       ]\n",
      " [0.09883601 0.09953443 0.09931419 0.097355   0.1       ]\n",
      " [0.10668575 0.10270912 0.10180736 0.11483609 0.1       ]\n",
      " [0.09832513 0.09932275 0.09954792 0.09627089 0.1       ]\n",
      " [0.09747092 0.09896735 0.0995387  0.09447277 0.1       ]\n",
      " [0.09489069 0.09788255 0.09929998 0.08915178 0.1       ]\n",
      " [0.09852217 0.09940447 0.09985881 0.09668824 0.1       ]]\n"
     ]
    }
   ],
   "source": [
    "# verify\n",
    "# X is (784, 10)\n",
    "# parameters is a dict\n",
    "# HL should be (10, 10)\n",
    "x_sample = train_set_x[:, 10:20]\n",
    "print(x_sample.shape)\n",
    "HL = L_layer_forward(x_sample, parameters=parameters)[0]\n",
    "print(HL[:, :5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss\n",
    "\n",
    "### compute_loss\n",
    "The next step is to compute the loss function after every forward pass to keep checking whether it is decreasing with training.<br> **compute_loss** here calculates the cross-entropy loss. You may want to use _[np.log()](https://docs.scipy.org/doc/numpy/reference/generated/numpy.log.html)_, _[np.sum()](https://docs.scipy.org/doc/numpy/reference/generated/numpy.log.html)_, _[np.multiply()](https://docs.scipy.org/doc/numpy/reference/generated/numpy.multiply.html)_ here. Do not forget that it is the average loss across all the data points in the batch. It takes the output of the last layer **HL** and the ground truth label **Y** as input and returns the **loss**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(HL, Y):\n",
    "\n",
    "\n",
    "    # HL is probability matrix of shape (10, number of examples)\n",
    "    # Y is true \"label\" vector shape (10, number of examples)\n",
    "\n",
    "    # loss is the cross-entropy loss\n",
    "\n",
    "    m = Y.shape[1]\n",
    "\n",
    "    loss = (-1./ m) * np.sum(np.multiply(Y, np.log(HL)))\n",
    "    \n",
    "    loss = np.squeeze(loss)      # To make sure that the loss's shape is what we expect (e.g. this turns [[17]] into 17).\n",
    "    assert(loss.shape == ())\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.4359949  0.02592623 0.54966248 0.43532239 0.4203678 ]\n",
      " [0.33033482 0.20464863 0.61927097 0.29965467 0.26682728]\n",
      " [0.62113383 0.52914209 0.13457995 0.51357812 0.18443987]\n",
      " [0.78533515 0.85397529 0.49423684 0.84656149 0.07964548]\n",
      " [0.50524609 0.0652865  0.42812233 0.09653092 0.12715997]\n",
      " [0.59674531 0.226012   0.10694568 0.22030621 0.34982629]\n",
      " [0.46778748 0.20174323 0.64040673 0.48306984 0.50523672]\n",
      " [0.38689265 0.79363745 0.58000418 0.1622986  0.70075235]\n",
      " [0.96455108 0.50000836 0.88952006 0.34161365 0.56714413]\n",
      " [0.42754596 0.43674726 0.77655918 0.53560417 0.95374223]]\n",
      "[[0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [1. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "0.8964600261334037\n"
     ]
    }
   ],
   "source": [
    "# sample\n",
    "# HL is (10, 5), Y is (10, 5)\n",
    "np.random.seed(2)\n",
    "HL_sample = np.random.rand(10,5)\n",
    "Y_sample = train_set_y[:, 10:15]\n",
    "print(HL_sample)\n",
    "print(Y_sample)\n",
    "\n",
    "print(compute_loss(HL_sample, Y_sample))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backpropagation\n",
    "Let's now get to the next step - backpropagation. Let's start with sigmoid_backward.\n",
    "\n",
    "### sigmoid-backward\n",
    "You might remember that we had created **sigmoid** function that calculated the activation for forward propagation. Now, we need the activation backward, which helps in calculating **dZ** from **dH**. Notice that it takes input **dH** and **sigmoid_memory** as input. **sigmoid_memory** is the **Z** which we had calculated during forward propagation. You use _[np.exp()](https://docs.scipy.org/doc/numpy/reference/generated/numpy.exp.html)_ here the following way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_backward(dH, sigmoid_memory):\n",
    "    \n",
    "    # Implement the backpropagation of a sigmoid function\n",
    "    # dH is gradient of the sigmoid activated activation of shape same as H or Z in the same layer    \n",
    "    # sigmoid_memory is the memory stored in the sigmoid(Z) calculation\n",
    "    \n",
    "    Z = sigmoid_memory\n",
    "    \n",
    "    H = 1/(1+np.exp(-Z))\n",
    "    dZ = dH * H * (1-H)\n",
    "    \n",
    "    assert (dZ.shape == Z.shape)\n",
    "    \n",
    "    return dZ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### relu-backward\n",
    "You might remember that we had created **relu** function that calculated the activation for forward propagation. Now, we need the activation backward, which helps in calculating **dZ** from **dH**. Notice that it takes input **dH** and **relu_memory** as input. **relu_memory** is the **Z** which we calculated uring forward propagation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu_backward(dH, relu_memory):\n",
    "    \n",
    "    # Implement the backpropagation of a relu function\n",
    "    # dH is gradient of the relu activated activation of shape same as H or Z in the same layer    \n",
    "    # relu_memory is the memory stored in the sigmoid(Z) calculation\n",
    "    \n",
    "    Z = relu_memory\n",
    "    dZ = np.array(dH, copy=True) # dZ will be the same as dA wherever the elements of A weren't 0\n",
    "    \n",
    "    dZ[Z <= 0] = 0\n",
    "    \n",
    "    assert (dZ.shape == Z.shape)\n",
    "    \n",
    "    return dZ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### layer_backward\n",
    "\n",
    "**layer_backward** is a complimentary function of **layer_forward**. Like **layer_forward** calculates **H** using **W**, **H_prev** and **b**, **layer_backward** uses **dH** to calculate **dW**, **dH_prev** and **db**. You have already studied the formulae in backpropogation. To calculate **dZ**, use the **sigmoid_backward** and **relu_backward** function. You might need to use _[np.dot()](https://docs.scipy.org/doc/numpy/reference/generated/numpy.dot.html)_, _[np.sum()](https://docs.scipy.org/doc/numpy/reference/generated/numpy.sum.html)_ for the rest. Remember to choose the axis correctly in db. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def layer_backward(dH, memory, activation = 'relu'):\n",
    "    \n",
    "    # takes dH and the memory calculated in layer_forward and activation as input to calculate the dH_prev, dW, db\n",
    "    # performs the backprop depending upon the activation function\n",
    "    \n",
    "\n",
    "    linear_memory, activation_memory = memory\n",
    "    \n",
    "    if activation == \"relu\":\n",
    "        dZ = relu_backward(dH, activation_memory)\n",
    "        H_prev, W, b = linear_memory\n",
    "        m = H_prev.shape[1]\n",
    "        dW = (1. / m) * np.dot(dZ, H_prev.T) \n",
    "        db = (1. / m) * np.sum(dZ, axis=1, keepdims=True)\n",
    "        dH_prev = np.dot(linear_memory[1].T, dZ)\n",
    "        \n",
    "    elif activation == \"sigmoid\":\n",
    "        dZ = sigmoid_backward(dH, activation_memory)\n",
    "        H_prev, W, b = linear_memory\n",
    "        m = H_prev.shape[1]\n",
    "        dW = (1. / m) * np.dot(dZ, H_prev.T) \n",
    "        db = (1. / m) * np.sum(dZ, axis=1, keepdims=True)\n",
    "        dH_prev = np.dot(linear_memory[1].T, dZ)\n",
    "    \n",
    "    return dH_prev, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dH_prev is \n",
      " [[5.6417525  0.66855959 6.86974666 5.46611139 4.92177244]\n",
      " [2.17997451 0.12963116 2.74831239 2.17661196 2.10183901]]\n",
      "dW is \n",
      " [[1.67565336 1.56891359]\n",
      " [1.39137819 1.4143854 ]\n",
      " [1.3597389  1.43013369]]\n",
      "db is \n",
      " [[0.37345476]\n",
      " [0.34414727]\n",
      " [0.29074635]]\n"
     ]
    }
   ],
   "source": [
    "# verify\n",
    "# l-1 has two neurons, l has three, m = 5\n",
    "# H_prev is (l-1, m)\n",
    "# W is (l, l-1)\n",
    "# b is (l, 1)\n",
    "# H should be (l, m)\n",
    "H_prev = np.array([[1,0, 5, 10, 2], [2, 5, 3, 10, 2]])\n",
    "W_sample = np.array([[10, 5], [2, 0], [1, 0]])\n",
    "b_sample = np.array([10, 5, 0]).reshape((3, 1))\n",
    "\n",
    "H, memory = layer_forward(H_prev, W_sample, b_sample, activation=\"relu\")\n",
    "np.random.seed(2)\n",
    "dH = np.random.rand(3,5)\n",
    "dH_prev, dW, db = layer_backward(dH, memory, activation = 'relu')\n",
    "print('dH_prev is \\n' , dH_prev)\n",
    "print('dW is \\n' ,dW)\n",
    "print('db is \\n', db)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### L_layer_backward\n",
    "\n",
    "**L_layer_backward** performs backpropagation for the whole network. Recall that the backpropagation for the last layer, i.e. the softmax layer, is different from the rest, hence it is outside the reversed `for` loop. You need to use the function **layer_backward** here in the loop with the activation function as **relu**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def L_layer_backward(HL, Y, memories):\n",
    "    \n",
    "    # Takes the predicted value HL and the true target value Y and the \n",
    "    # memories calculated by L_layer_forward as input\n",
    "    \n",
    "    # returns the gradients calulated for all the layers as a dict\n",
    "\n",
    "    gradients = {}\n",
    "    L = len(memories) # the number of layers\n",
    "    m = HL.shape[1]\n",
    "    Y = Y.reshape(HL.shape) # after this line, Y is the same shape as AL\n",
    "    \n",
    "    # Perform the backprop for the last layer that is the softmax layer\n",
    "    current_memory = memories[-1]\n",
    "    linear_memory, activation_memory = current_memory\n",
    "    dZ = HL - Y\n",
    "    H_prev, W, b = linear_memory\n",
    "    gradients[\"dH\" + str(L-1)] = np.dot(linear_memory[1].T, dZ)\n",
    "    gradients[\"dW\" + str(L)] = (1. / m) * np.dot(dZ, H_prev.T) \n",
    "    gradients[\"db\" + str(L)] = (1. / m) * np.sum(dZ, axis=1, keepdims=True)\n",
    "    \n",
    "    # Perform the backpropagation l-1 times\n",
    "    for l in reversed(range(L-1)):\n",
    "        # Lth layer gradients: \"gradients[\"dH\" + str(l + 1)] \", gradients[\"dW\" + str(l + 2)] , gradients[\"db\" + str(l + 2)]\n",
    "        current_memory = memories[l]\n",
    "        \n",
    "        dH_prev_temp, dW_temp, db_temp = layer_backward(gradients[\"dH\" + str(l + 1)], current_memory, activation=\"relu\")\n",
    "        gradients[\"dH\" + str(l)] = dH_prev_temp\n",
    "        gradients[\"dW\" + str(l + 1)] = dW_temp\n",
    "        gradients[\"db\" + str(l + 1)] = db_temp\n",
    "\n",
    "\n",
    "    return gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dW3 is \n",
      " [[ 0.02003701  0.0019043   0.01011729  0.0145757   0.00146444  0.00059863\n",
      "   0.        ]\n",
      " [ 0.02154547  0.00203519  0.01085648  0.01567075  0.00156469  0.00060533\n",
      "   0.        ]\n",
      " [-0.01718407 -0.00273711 -0.00499101 -0.00912135 -0.00207365  0.00059996\n",
      "   0.        ]\n",
      " [-0.01141498 -0.00158622 -0.00607049 -0.00924709 -0.00119619  0.00060381\n",
      "   0.        ]\n",
      " [ 0.01943173  0.0018421   0.00984543  0.01416368  0.00141676  0.00059682\n",
      "   0.        ]\n",
      " [ 0.01045447  0.00063974  0.00637621  0.00863306  0.00050118  0.00060441\n",
      "   0.        ]\n",
      " [-0.06338911 -0.00747251 -0.0242169  -0.03835708 -0.00581131  0.0006034\n",
      "   0.        ]\n",
      " [ 0.01911373  0.001805    0.00703101  0.0120636   0.00138836 -0.00140535\n",
      "   0.        ]\n",
      " [-0.01801603  0.0017357  -0.01489228 -0.02026076  0.00133528  0.00060264\n",
      "   0.        ]\n",
      " [ 0.0194218   0.00183381  0.00594427  0.01187949  0.00141043 -0.00340965\n",
      "   0.        ]]\n",
      "db3 is \n",
      " [[ 0.10031756]\n",
      " [ 0.00460183]\n",
      " [-0.00142942]\n",
      " [-0.0997827 ]\n",
      " [ 0.09872663]\n",
      " [ 0.00536378]\n",
      " [-0.10124784]\n",
      " [-0.00191121]\n",
      " [-0.00359044]\n",
      " [-0.00104818]]\n",
      "dW2 is \n",
      " [[ 4.94428956e-05  1.13215514e-02  5.44180380e-02]\n",
      " [-4.81267081e-05 -2.96999448e-05 -1.81899582e-02]\n",
      " [ 5.63424333e-05  4.77190073e-03  4.04810232e-02]\n",
      " [ 1.49767478e-04 -1.89780927e-03 -7.91231369e-03]\n",
      " [ 1.97866094e-04  1.22107085e-04  2.64140566e-02]\n",
      " [ 0.00000000e+00 -3.75805770e-04  1.63906102e-05]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00]]\n",
      "db2 is \n",
      " [[ 0.013979  ]\n",
      " [-0.01329383]\n",
      " [ 0.01275707]\n",
      " [-0.01052957]\n",
      " [ 0.03179224]\n",
      " [-0.00039877]\n",
      " [ 0.        ]]\n"
     ]
    }
   ],
   "source": [
    "# verify\n",
    "# X is (784, 10)\n",
    "# parameters is a dict\n",
    "# HL should be (10, 10)\n",
    "x_sample = train_set_x[:, 10:20]\n",
    "y_sample = train_set_y[:, 10:20]\n",
    "\n",
    "HL, memories = L_layer_forward(x_sample, parameters=parameters)\n",
    "gradients  = L_layer_backward(HL, y_sample, memories)\n",
    "print('dW3 is \\n', gradients['dW3'])\n",
    "print('db3 is \\n', gradients['db3'])\n",
    "print('dW2 is \\n', gradients['dW2'])\n",
    "print('db2 is \\n', gradients['db2'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameter Updates\n",
    "\n",
    "Now that we have calculated the gradients. let's do the last step which is updating the weights and biases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters(parameters, gradients, learning_rate):\n",
    "\n",
    "    # parameters is the python dictionary containing the parameters W and b for all the layers\n",
    "    # gradients is the python dictionary containing your gradients, output of L_model_backward\n",
    "    \n",
    "    # returns updated weights after applying the gradient descent update\n",
    "\n",
    "    \n",
    "    L = len(parameters) // 2 # number of layers in the neural network\n",
    "\n",
    "    for l in range(L):\n",
    "        parameters[\"W\" + str(l+1)] = parameters[\"W\" + str(l+1)] - learning_rate * gradients[\"dW\" + str(l+1)]\n",
    "        parameters[\"b\" + str(l+1)] = parameters[\"b\" + str(l+1)] - learning_rate * gradients[\"db\" + str(l+1)]\n",
    "\n",
    "        \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having defined the bits and pieces of the feedforward and the backpropagation, let's now combine all that to form a model. The list `dimensions` has the number of neurons in each layer specified in it. For a neural network with 1 hidden layer with 45 neurons, you would specify the dimensions as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "dimensions = [784, 45, 10] #  three-layer model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model\n",
    "\n",
    "### L_layer_model\n",
    "\n",
    "This is a composite function which takes the training data as input **X**, ground truth label **Y**, the **dimensions** as stated above, **learning_rate**, the number of iterations **num_iterations** and if you want to print the loss, **print_loss**. You need to use the final functions we have use for feedforward, computing the loss, backpropagation and updating the parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: L_layer_model\n",
    "\n",
    "def L_layer_model(X, Y, dimensions, learning_rate = 0.0075, num_iterations = 3000, print_loss=False):\n",
    "    \n",
    "    # X and Y are the input training datasets\n",
    "    # learning_rate, num_iterations are gradient descent optimization parameters\n",
    "    # returns updated parameters\n",
    "\n",
    "    np.random.seed(2)\n",
    "    losses = []                         # keep track of loss\n",
    "    \n",
    "    # Parameters initialization\n",
    "    parameters = initialize_parameters(dimensions)\n",
    " \n",
    "    for i in range(0, num_iterations):\n",
    "\n",
    "        # Forward propagation\n",
    "        HL, memories = L_layer_forward(X, parameters)\n",
    "        \n",
    "        # Compute loss\n",
    "        loss = compute_loss(HL, Y)\n",
    "    \n",
    "        # Backward propagation\n",
    "        gradients = L_layer_backward(HL, Y, memories)\n",
    " \n",
    "        # Update parameters.\n",
    "        parameters = update_parameters(parameters, gradients, learning_rate)\n",
    "                \n",
    "        # Printing the loss every 100 training example\n",
    "        if print_loss and i % 100 == 0:\n",
    "            print (\"Loss after iteration %i: %f\" %(i, loss))\n",
    "            losses.append(loss)\n",
    "            \n",
    "    # plotting the loss\n",
    "    plt.plot(np.squeeze(losses))\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('iterations (per tens)')\n",
    "    plt.title(\"Learning rate =\" + str(learning_rate))\n",
    "    plt.show()\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(784, 50000)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set_x_new = train_set_x[:,0:50000]\n",
    "train_set_y_new = train_set_y[:,0:50000]\n",
    "train_set_x_new.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's call the function L_layer_model on the dataset we have created. This will take 250 mins to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after iteration 0: 2.424679\n",
      "Loss after iteration 100: 2.140705\n",
      "Loss after iteration 200: 1.902172\n",
      "Loss after iteration 300: 1.642645\n",
      "Loss after iteration 400: 1.394668\n",
      "Loss after iteration 500: 1.190004\n",
      "Loss after iteration 600: 1.034682\n",
      "Loss after iteration 700: 0.919556\n",
      "Loss after iteration 800: 0.833380\n",
      "Loss after iteration 900: 0.767389\n",
      "Loss after iteration 1000: 0.715534\n",
      "Loss after iteration 1100: 0.673834\n",
      "Loss after iteration 1200: 0.639608\n",
      "Loss after iteration 1300: 0.611036\n",
      "Loss after iteration 1400: 0.586827\n",
      "Loss after iteration 1500: 0.566057\n",
      "Loss after iteration 1600: 0.548020\n",
      "Loss after iteration 1700: 0.532194\n",
      "Loss after iteration 1800: 0.518209\n",
      "Loss after iteration 1900: 0.505744\n",
      "Loss after iteration 2000: 0.494555\n",
      "Loss after iteration 2100: 0.484444\n",
      "Loss after iteration 2200: 0.475262\n",
      "Loss after iteration 2300: 0.466881\n",
      "Loss after iteration 2400: 0.459192\n",
      "Loss after iteration 2500: 0.452107\n",
      "Loss after iteration 2600: 0.445552\n",
      "Loss after iteration 2700: 0.439460\n",
      "Loss after iteration 2800: 0.433784\n",
      "Loss after iteration 2900: 0.428477\n",
      "Loss after iteration 3000: 0.423498\n",
      "Loss after iteration 3100: 0.418815\n",
      "Loss after iteration 3200: 0.414397\n",
      "Loss after iteration 3300: 0.410220\n",
      "Loss after iteration 3400: 0.406262\n",
      "Loss after iteration 3500: 0.402503\n",
      "Loss after iteration 3600: 0.398925\n",
      "Loss after iteration 3700: 0.395516\n",
      "Loss after iteration 3800: 0.392262\n",
      "Loss after iteration 3900: 0.389151\n",
      "Loss after iteration 4000: 0.386171\n",
      "Loss after iteration 4100: 0.383313\n",
      "Loss after iteration 4200: 0.380569\n",
      "Loss after iteration 4300: 0.377931\n",
      "Loss after iteration 4400: 0.375388\n",
      "Loss after iteration 4500: 0.372936\n",
      "Loss after iteration 4600: 0.370570\n",
      "Loss after iteration 4700: 0.368285\n",
      "Loss after iteration 4800: 0.366075\n",
      "Loss after iteration 4900: 0.363934\n",
      "Loss after iteration 5000: 0.361857\n",
      "Loss after iteration 5100: 0.359842\n",
      "Loss after iteration 5200: 0.357884\n",
      "Loss after iteration 5300: 0.355982\n",
      "Loss after iteration 5400: 0.354132\n",
      "Loss after iteration 5500: 0.352332\n",
      "Loss after iteration 5600: 0.350577\n",
      "Loss after iteration 5700: 0.348865\n",
      "Loss after iteration 5800: 0.347194\n",
      "Loss after iteration 5900: 0.345563\n",
      "Loss after iteration 6000: 0.343970\n",
      "Loss after iteration 6100: 0.342413\n",
      "Loss after iteration 6200: 0.340889\n",
      "Loss after iteration 6300: 0.339396\n",
      "Loss after iteration 6400: 0.337933\n",
      "Loss after iteration 6500: 0.336498\n",
      "Loss after iteration 6600: 0.335092\n",
      "Loss after iteration 6700: 0.333714\n",
      "Loss after iteration 6800: 0.332361\n",
      "Loss after iteration 6900: 0.331033\n",
      "Loss after iteration 7000: 0.329727\n",
      "Loss after iteration 7100: 0.328441\n",
      "Loss after iteration 7200: 0.327178\n",
      "Loss after iteration 7300: 0.325934\n",
      "Loss after iteration 7400: 0.324709\n",
      "Loss after iteration 7500: 0.323505\n",
      "Loss after iteration 7600: 0.322322\n",
      "Loss after iteration 7700: 0.321156\n",
      "Loss after iteration 7800: 0.320007\n",
      "Loss after iteration 7900: 0.318878\n",
      "Loss after iteration 8000: 0.317767\n",
      "Loss after iteration 8100: 0.316670\n",
      "Loss after iteration 8200: 0.315588\n",
      "Loss after iteration 8300: 0.314521\n",
      "Loss after iteration 8400: 0.313467\n",
      "Loss after iteration 8500: 0.312425\n",
      "Loss after iteration 8600: 0.311394\n",
      "Loss after iteration 8700: 0.310374\n",
      "Loss after iteration 8800: 0.309366\n",
      "Loss after iteration 8900: 0.308370\n",
      "Loss after iteration 9000: 0.307387\n",
      "Loss after iteration 9100: 0.306413\n",
      "Loss after iteration 9200: 0.305448\n",
      "Loss after iteration 9300: 0.304493\n",
      "Loss after iteration 9400: 0.303550\n",
      "Loss after iteration 9500: 0.302616\n",
      "Loss after iteration 9600: 0.301694\n",
      "Loss after iteration 9700: 0.300784\n",
      "Loss after iteration 9800: 0.299884\n",
      "Loss after iteration 9900: 0.298993\n",
      "Loss after iteration 10000: 0.298111\n",
      "Loss after iteration 10100: 0.297238\n",
      "Loss after iteration 10200: 0.296371\n",
      "Loss after iteration 10300: 0.295513\n",
      "Loss after iteration 10400: 0.294663\n",
      "Loss after iteration 10500: 0.293820\n",
      "Loss after iteration 10600: 0.292985\n",
      "Loss after iteration 10700: 0.292155\n",
      "Loss after iteration 10800: 0.291335\n",
      "Loss after iteration 10900: 0.290521\n",
      "Loss after iteration 11000: 0.289713\n",
      "Loss after iteration 11100: 0.288914\n",
      "Loss after iteration 11200: 0.288121\n",
      "Loss after iteration 11300: 0.287334\n",
      "Loss after iteration 11400: 0.286553\n",
      "Loss after iteration 11500: 0.285778\n",
      "Loss after iteration 11600: 0.285009\n",
      "Loss after iteration 11700: 0.284244\n",
      "Loss after iteration 11800: 0.283485\n",
      "Loss after iteration 11900: 0.282730\n",
      "Loss after iteration 12000: 0.281981\n",
      "Loss after iteration 12100: 0.281237\n",
      "Loss after iteration 12200: 0.280499\n",
      "Loss after iteration 12300: 0.279766\n",
      "Loss after iteration 12400: 0.279039\n",
      "Loss after iteration 12500: 0.278318\n",
      "Loss after iteration 12600: 0.277602\n",
      "Loss after iteration 12700: 0.276891\n",
      "Loss after iteration 12800: 0.276184\n",
      "Loss after iteration 12900: 0.275482\n",
      "Loss after iteration 13000: 0.274785\n",
      "Loss after iteration 13100: 0.274091\n",
      "Loss after iteration 13200: 0.273400\n",
      "Loss after iteration 13300: 0.272713\n",
      "Loss after iteration 13400: 0.272032\n",
      "Loss after iteration 13500: 0.271356\n",
      "Loss after iteration 13600: 0.270685\n",
      "Loss after iteration 13700: 0.270018\n",
      "Loss after iteration 13800: 0.269353\n",
      "Loss after iteration 13900: 0.268691\n",
      "Loss after iteration 14000: 0.268034\n",
      "Loss after iteration 14100: 0.267380\n",
      "Loss after iteration 14200: 0.266729\n",
      "Loss after iteration 14300: 0.266082\n",
      "Loss after iteration 14400: 0.265437\n",
      "Loss after iteration 14500: 0.264795\n",
      "Loss after iteration 14600: 0.264157\n",
      "Loss after iteration 14700: 0.263525\n",
      "Loss after iteration 14800: 0.262896\n",
      "Loss after iteration 14900: 0.262270\n",
      "Loss after iteration 15000: 0.261648\n",
      "Loss after iteration 15100: 0.261028\n",
      "Loss after iteration 15200: 0.260412\n",
      "Loss after iteration 15300: 0.259797\n",
      "Loss after iteration 15400: 0.259186\n",
      "Loss after iteration 15500: 0.258578\n",
      "Loss after iteration 15600: 0.257975\n",
      "Loss after iteration 15700: 0.257375\n",
      "Loss after iteration 15800: 0.256777\n",
      "Loss after iteration 15900: 0.256182\n",
      "Loss after iteration 16000: 0.255591\n",
      "Loss after iteration 16100: 0.255002\n",
      "Loss after iteration 16200: 0.254415\n",
      "Loss after iteration 16300: 0.253831\n",
      "Loss after iteration 16400: 0.253250\n",
      "Loss after iteration 16500: 0.252671\n",
      "Loss after iteration 16600: 0.252095\n",
      "Loss after iteration 16700: 0.251522\n",
      "Loss after iteration 16800: 0.250950\n",
      "Loss after iteration 16900: 0.250381\n",
      "Loss after iteration 17000: 0.249814\n",
      "Loss after iteration 17100: 0.249249\n",
      "Loss after iteration 17200: 0.248686\n",
      "Loss after iteration 17300: 0.248125\n",
      "Loss after iteration 17400: 0.247567\n",
      "Loss after iteration 17500: 0.247010\n",
      "Loss after iteration 17600: 0.246456\n",
      "Loss after iteration 17700: 0.245904\n",
      "Loss after iteration 17800: 0.245355\n",
      "Loss after iteration 17900: 0.244808\n",
      "Loss after iteration 18000: 0.244263\n",
      "Loss after iteration 18100: 0.243721\n",
      "Loss after iteration 18200: 0.243182\n",
      "Loss after iteration 18300: 0.242644\n",
      "Loss after iteration 18400: 0.242110\n",
      "Loss after iteration 18500: 0.241578\n",
      "Loss after iteration 18600: 0.241049\n",
      "Loss after iteration 18700: 0.240523\n",
      "Loss after iteration 18800: 0.239999\n",
      "Loss after iteration 18900: 0.239477\n",
      "Loss after iteration 19000: 0.238957\n",
      "Loss after iteration 19100: 0.238439\n",
      "Loss after iteration 19200: 0.237923\n",
      "Loss after iteration 19300: 0.237409\n",
      "Loss after iteration 19400: 0.236897\n",
      "Loss after iteration 19500: 0.236387\n",
      "Loss after iteration 19600: 0.235879\n",
      "Loss after iteration 19700: 0.235374\n",
      "Loss after iteration 19800: 0.234872\n",
      "Loss after iteration 19900: 0.234372\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "parameters = L_layer_model(train_set_x_new, train_set_y_new, dimensions, num_iterations = 20000, print_loss = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X, y, parameters):\n",
    "    \n",
    "    # Performs forward propogation using the trained parameters and calculates the accuracy\n",
    "    \n",
    "    m = X.shape[1]\n",
    "    n = len(parameters) // 2 # number of layers in the neural network\n",
    "    \n",
    "    # Forward propagation\n",
    "    probas, caches = L_layer_forward(X, parameters)\n",
    "    \n",
    "    p = np.argmax(probas, axis = 0)\n",
    "    act = np.argmax(y, axis = 0)\n",
    "\n",
    "    print(\"Accuracy: \"  + str(np.sum((p == act)/m)))\n",
    "        \n",
    "    return p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see the accuray we get on the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.93342\n"
     ]
    }
   ],
   "source": [
    "pred_train = predict(train_set_x_new, train_set_y_new, parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9338000000000003\n"
     ]
    }
   ],
   "source": [
    "pred_test = predict(test_set_x, test_set_y, parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x18afe7f8128>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "index  = 3450\n",
    "k = test_set_x[:,index]\n",
    "k = k.reshape((28, 28))\n",
    "plt.title('Label is {label}'.format(label=(pred_test[index], np.argmax(test_set_y, axis = 0)[index])))\n",
    "plt.imshow(k, cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
